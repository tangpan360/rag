{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第23章：综合项目 - 智能问答系统\n",
        "\n",
        "## 项目概述\n",
        "\n",
        "本章将构建一个**完整的企业文档智能问答系统**，整合前面学习的所有知识：\n",
        "\n",
        "### 🎯 项目目标\n",
        "\n",
        "构建一个基于RAG（Retrieval Augmented Generation）的问答系统，能够：\n",
        "- 📄 加载和处理文档\n",
        "- 🔍 智能检索相关内容\n",
        "- 💬 生成准确的答案\n",
        "- 📝 引用信息来源\n",
        "- ⚡ 支持流式输出\n",
        "- 📊 评估系统性能\n",
        "\n",
        "### 🛠 技术栈\n",
        "\n",
        "| 组件 | 技术 | 作用 |\n",
        "|------|------|------|\n",
        "| **文档加载** | Document Loaders | 读取各种格式文档 |\n",
        "| **文本分割** | Text Splitters | 将文档切分成chunks |\n",
        "| **向量化** | Embeddings | 将文本转换为向量 |\n",
        "| **向量存储** | Vector Store | 存储和检索向量 |\n",
        "| **检索** | Retriever | 查找相关文档 |\n",
        "| **生成** | RAG Agent | 基于检索内容生成答案 |\n",
        "| **评估** | Evaluators | 评估答案质量 |\n",
        "\n",
        "### 📋 系统架构\n",
        "\n",
        "```\n",
        "文档输入\n",
        "  ↓\n",
        "文档加载器 (Document Loader)\n",
        "  ↓\n",
        "文本分割器 (Text Splitter)\n",
        "  ↓\n",
        "向量化 (Embeddings)\n",
        "  ↓\n",
        "向量存储 (Vector Store)\n",
        "  ↓\n",
        "用户提问 → 检索器 (Retriever) → RAG Agent → 答案输出\n",
        "                                      ↓\n",
        "                                   引用来源\n",
        "```\n",
        "\n",
        "### 💡 核心特性\n",
        "\n",
        "1. **多格式支持** - 支持TXT、Markdown等文本格式\n",
        "2. **智能检索** - 基于语义相似度检索\n",
        "3. **引用来源** - 答案附带文档来源\n",
        "4. **流式输出** - 实时显示生成过程\n",
        "5. **质量评估** - 自动评估答案质量\n",
        "\n",
        "---\n",
        "\n",
        "## 学习收获\n",
        "\n",
        "通过本项目，你将学会：\n",
        "- ✅ 如何构建完整的RAG应用\n",
        "- ✅ 如何选择和配置各个组件\n",
        "- ✅ 如何优化检索和生成质量\n",
        "- ✅ 如何评估系统性能\n",
        "- ✅ 如何处理实际场景中的问题\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 环境配置完成！\n",
            "项目根目录: /home/iip/tp/038-project/rag\n",
            "\n",
            "📦 已导入的核心组件：\n",
            "  - ChatOpenAI: 大语言模型\n",
            "  - OpenAIEmbeddings: 文本向量化\n",
            "  - TextLoader: 文档加载器\n",
            "  - RecursiveCharacterTextSplitter: 文本分割器\n",
            "  - InMemoryVectorStore: 向量存储\n",
            "  - create_agent: Agent创建\n",
            "\n",
            "🚀 准备构建智能问答系统！\n"
          ]
        }
      ],
      "source": [
        "# 环境配置和依赖导入\n",
        "import os\n",
        "import sys\n",
        "from importlib import reload\n",
        "\n",
        "_project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "sys.path.append(_project_root)\n",
        "\n",
        "from config import config\n",
        "\n",
        "# 导入LangChain核心组件\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "\n",
        "print(\"✅ 环境配置完成！\")\n",
        "print(f\"项目根目录: {_project_root}\")\n",
        "print(\"\\n📦 已导入的核心组件：\")\n",
        "print(\"  - ChatOpenAI: 大语言模型\")\n",
        "print(\"  - OpenAIEmbeddings: 文本向量化\")\n",
        "print(\"  - TextLoader: 文档加载器\")\n",
        "print(\"  - RecursiveCharacterTextSplitter: 文本分割器\")\n",
        "print(\"  - InMemoryVectorStore: 向量存储\")\n",
        "print(\"  - create_agent: Agent创建\")\n",
        "print(\"\\n🚀 准备构建智能问答系统！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤1：文档加载与处理\n",
        "\n",
        "加载文档并分割成适合检索的chunks。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤1：加载和分割文档】\n",
            "\n",
            "=== 加载文档 ===\n",
            "\n",
            "✓ 加载文档: /home/iip/tp/038-project/rag/docs/sample_doc.txt\n",
            "  文档数量: 1\n",
            "  文档长度: 724 字符\n",
            "  文档预览: # LangChain 学习指南\n",
            "\n",
            "## 什么是LangChain？\n",
            "\n",
            "LangChain是一个强大的开源框架，专门用于构建基于大语言模型（LLM）的应用程序。它提供了一套完整的工具和抽象，使开发者能...\n",
            "\n",
            "=== 分割文档 ===\n",
            "\n",
            "✓ 文档分割完成\n",
            "  分割后的chunks数量: 2\n",
            "\n",
            "前3个chunks预览：\n",
            "\n",
            "  Chunk 1 (460 字符):\n",
            "  # LangChain 学习指南\n",
            "\n",
            "## 什么是LangChain？\n",
            "\n",
            "LangChain是一个强大的开源框架，专门用于构建基于大语言模型（LLM）的应用程序。...\n",
            "\n",
            "  Chunk 2 (302 字符):\n",
            "  检索增强生成（RAG）是LangChain的核心应用之一。它通过以下步骤工作：\n",
            "\n",
            "1. 文档加载：使用Document Loaders读取各种格式的文档\n",
            "2. ...\n",
            "\n",
            "💡 分割策略：\n",
            "  - chunk_size=500: 平衡上下文和检索精度\n",
            "  - chunk_overlap=50: 保持语义连贯性\n",
            "  - 按段落、句子分割: 保持语义完整\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤1：加载和分割文档】\\n\")\n",
        "\n",
        "# 1. 加载文档\n",
        "print(\"=== 加载文档 ===\\n\")\n",
        "doc_path = os.path.join(_project_root, \"docs\", \"sample_doc.txt\")\n",
        "\n",
        "loader = TextLoader(doc_path, encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"✓ 加载文档: {doc_path}\")\n",
        "print(f\"  文档数量: {len(documents)}\")\n",
        "print(f\"  文档长度: {len(documents[0].page_content)} 字符\")\n",
        "print(f\"  文档预览: {documents[0].page_content[:100]}...\")\n",
        "print()\n",
        "\n",
        "# 2. 分割文档\n",
        "print(\"=== 分割文档 ===\\n\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,      # 每个chunk最多500字符\n",
        "    chunk_overlap=50,    # chunk之间重叠50字符\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"✓ 文档分割完成\")\n",
        "print(f\"  分割后的chunks数量: {len(doc_splits)}\")\n",
        "print(f\"\\n前3个chunks预览：\")\n",
        "for i, chunk in enumerate(doc_splits[:3], 1):\n",
        "    print(f\"\\n  Chunk {i} ({len(chunk.page_content)} 字符):\")\n",
        "    print(f\"  {chunk.page_content[:80]}...\")\n",
        "\n",
        "print(f\"\\n💡 分割策略：\")\n",
        "print(f\"  - chunk_size=500: 平衡上下文和检索精度\")\n",
        "print(f\"  - chunk_overlap=50: 保持语义连贯性\")\n",
        "print(f\"  - 按段落、句子分割: 保持语义完整\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤2：向量化与存储\n",
        "\n",
        "将文档chunks转换为向量并存储，支持语义检索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤2：向量化与存储】\n",
            "\n",
            "=== 初始化Embeddings模型 ===\n",
            "\n",
            "✓ Embeddings模型: text-embedding-3-small\n",
            "\n",
            "=== 创建向量存储 ===\n",
            "\n",
            "✓ 向量存储创建完成\n",
            "  已索引 2 个文档chunks\n",
            "\n",
            "=== 测试检索功能 ===\n",
            "\n",
            "测试查询: 什么是LangChain？\n",
            "\n",
            "检索到 2 个相关chunks：\n",
            "\n",
            "Chunk 1:\n",
            "  内容: # LangChain 学习指南\n",
            "\n",
            "## 什么是LangChain？\n",
            "\n",
            "LangChain是一个强大的开源框架，专门用于构建基于大语言模型（LLM）的应用程序。它提供了一套完整的工具和抽象，使开发者能...\n",
            "  长度: 460 字符\n",
            "\n",
            "Chunk 2:\n",
            "  内容: 检索增强生成（RAG）是LangChain的核心应用之一。它通过以下步骤工作：\n",
            "\n",
            "1. 文档加载：使用Document Loaders读取各种格式的文档\n",
            "2. 文本分割：将长文档切分成小块\n",
            "3. 向量...\n",
            "  长度: 302 字符\n",
            "\n",
            "💡 检索工作原理：\n",
            "  1. 将查询转换为向量\n",
            "  2. 计算与所有chunks的相似度\n",
            "  3. 返回最相似的top-k个chunks\n",
            "  4. 这些chunks将用于生成答案\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤2：向量化与存储】\\n\")\n",
        "\n",
        "# 初始化Embeddings模型\n",
        "print(\"=== 初始化Embeddings模型 ===\\n\")\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=config.CLOUD_API_KEY,\n",
        "    base_url=config.CLOUD_BASE_URL\n",
        ")\n",
        "print(f\"✓ Embeddings模型: {embeddings.model}\")\n",
        "print()\n",
        "\n",
        "# 创建向量存储并索引文档\n",
        "print(\"=== 创建向量存储 ===\\n\")\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=embeddings\n",
        ")\n",
        "print(f\"✓ 向量存储创建完成\")\n",
        "print(f\"  已索引 {len(doc_splits)} 个文档chunks\")\n",
        "print()\n",
        "\n",
        "# 测试检索功能\n",
        "print(\"=== 测试检索功能 ===\\n\")\n",
        "test_query = \"什么是LangChain？\"\n",
        "print(f\"测试查询: {test_query}\\n\")\n",
        "\n",
        "# 检索最相关的3个chunks\n",
        "retrieved_docs = vectorstore.similarity_search(test_query, k=3)\n",
        "\n",
        "print(f\"检索到 {len(retrieved_docs)} 个相关chunks：\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    print(f\"  内容: {doc.page_content[:100]}...\")\n",
        "    print(f\"  长度: {len(doc.page_content)} 字符\\n\")\n",
        "\n",
        "print(\"💡 检索工作原理：\")\n",
        "print(\"  1. 将查询转换为向量\")\n",
        "print(\"  2. 计算与所有chunks的相似度\")\n",
        "print(\"  3. 返回最相似的top-k个chunks\")\n",
        "print(\"  4. 这些chunks将用于生成答案\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤3：构建RAG Agent\n",
        "\n",
        "创建一个智能Agent，能够检索文档并生成答案。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤3：构建RAG Agent】\n",
            "\n",
            "=== 创建检索工具 ===\n",
            "\n",
            "✓ 创建检索工具: retrieve_documents\n",
            "  - 检索top-3相关文档\n",
            "  - 返回格式化内容和原始文档\n",
            "\n",
            "=== 初始化LLM ===\n",
            "\n",
            "✓ 模型: gpt-4.1-mini\n",
            "\n",
            "=== 创建RAG Agent ===\n",
            "\n",
            "✓ RAG Agent创建完成\n",
            "  - 工具: retrieve_documents\n",
            "  - 系统提示: 基于文档回答问题\n",
            "\n",
            "🎉 RAG系统构建完成！\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤3：构建RAG Agent】\\n\")\n",
        "\n",
        "# 1. 创建检索工具\n",
        "print(\"=== 创建检索工具 ===\\n\")\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_documents(query: str):\n",
        "    \"\"\"检索与查询相关的文档内容\"\"\"\n",
        "    # 检索top-3相关文档\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    \n",
        "    # 格式化文档内容\n",
        "    formatted_docs = \"\\n\\n\".join([\n",
        "        f\"【文档片段 {i+1}】\\n{doc.page_content}\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    ])\n",
        "    \n",
        "    # 返回格式化内容和原始文档（artifact）\n",
        "    return formatted_docs, docs\n",
        "\n",
        "print(\"✓ 创建检索工具: retrieve_documents\")\n",
        "print(\"  - 检索top-3相关文档\")\n",
        "print(\"  - 返回格式化内容和原始文档\")\n",
        "print()\n",
        "\n",
        "# 2. 初始化模型\n",
        "print(\"=== 初始化LLM ===\\n\")\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    temperature=0,\n",
        "    api_key=config.CLOUD_API_KEY,\n",
        "    base_url=config.CLOUD_BASE_URL,\n",
        ")\n",
        "print(f\"✓ 模型: {llm.model_name}\")\n",
        "print()\n",
        "\n",
        "# 3. 创建RAG Agent\n",
        "print(\"=== 创建RAG Agent ===\\n\")\n",
        "rag_agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[retrieve_documents],\n",
        "    system_prompt=\"\"\"你是一个专业的文档问答助手。\n",
        "\n",
        "工作流程：\n",
        "1. 使用retrieve_documents工具检索相关文档\n",
        "2. 仔细阅读检索到的文档内容\n",
        "3. 基于文档内容回答用户问题\n",
        "4. 如果文档中没有相关信息，明确告知用户\n",
        "\n",
        "要求：\n",
        "- 答案必须基于检索到的文档内容\n",
        "- 保持简洁，3-5句话\n",
        "- 如果有多个相关信息，综合回答\n",
        "- 不要编造文档中没有的信息\"\"\"\n",
        ")\n",
        "\n",
        "print(\"✓ RAG Agent创建完成\")\n",
        "print(\"  - 工具: retrieve_documents\")\n",
        "print(\"  - 系统提示: 基于文档回答问题\")\n",
        "print(\"\\n🎉 RAG系统构建完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤4：测试RAG系统\n",
        "\n",
        "测试系统的问答能力和引用功能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤4：测试RAG系统】\n",
            "\n",
            "=== 测试问答功能 ===\n",
            "\n",
            "【问题 1】什么是LangChain？\n",
            "\n",
            "回答: LangChain是一个开源框架，专门用于构建基于大语言模型（LLM）的应用程序。它提供了模型、提示词、链、代理和记忆等核心组件，方便开发者集成和管理LLM。LangChain的核心应用之一是检索增强生成（RAG），通过文档加载、文本分割、向量化、存储、检索和生成步骤，实现基于文档的智能问答和内容生成。常见应用包括智能客服、文档问答、代码助手和内容生成工具。\n",
            "\n",
            "✓ 使用了检索工具\n",
            "  检索了 2 个文档片段\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "【问题 2】LangChain有哪些核心组件？\n",
            "\n",
            "回答: LangChain的核心组件包括：1. Models（模型），支持多种大语言模型提供商；2. Prompts（提示词），提供提示词模板和管理工具；3. Chains（链），用于将多个组件串联形成复杂流程；4. Agents（代理），根据输入动态调用工具实现智能交互；5. Memory（记忆），为对话系统提供上下文记忆能力。\n",
            "\n",
            "✓ 使用了检索工具\n",
            "  检索了 2 个文档片段\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "【问题 3】RAG技术是如何工作的？\n",
            "\n",
            "回答: RAG（检索增强生成）技术的工作流程包括：首先使用文档加载器读取各种格式的文档，然后将长文档切分成小块，接着将文本转换为向量表示并存储在向量数据库中。用户提出问题时，系统会检索相关文档内容，最后基于检索到的内容生成答案。该技术广泛应用于智能客服、文档问答、代码助手等场景。\n",
            "\n",
            "✓ 使用了检索工具\n",
            "  检索了 2 个文档片段\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "【问题 4】LangChain适合什么使用场景？\n",
            "\n",
            "回答: LangChain适合用于构建基于大语言模型的各种应用，典型使用场景包括智能客服系统、文档问答系统、代码助手、内容生成工具和数据分析助手。它通过检索增强生成（RAG）技术，实现文档加载、文本分割、向量化、存储、检索和生成的完整流程，支持复杂的多组件链式处理和动态代理调用，适合需要集成LLM和上下文记忆的场景。\n",
            "\n",
            "✓ 使用了检索工具\n",
            "  检索了 2 个文档片段\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "💡 观察要点：\n",
            "  ✓ Agent自动决定何时使用检索工具\n",
            "  ✓ 答案基于检索到的文档内容\n",
            "  ✓ 可以通过artifact获取原始文档\n",
            "  ✓ 所有执行过程都被LangSmith追踪\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤4：测试RAG系统】\\n\")\n",
        "\n",
        "# 定义测试问题\n",
        "test_questions = [\n",
        "    \"什么是LangChain？\",\n",
        "    \"LangChain有哪些核心组件？\",\n",
        "    \"RAG技术是如何工作的？\",\n",
        "    \"LangChain适合什么使用场景？\"\n",
        "]\n",
        "\n",
        "print(\"=== 测试问答功能 ===\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"【问题 {i}】{question}\\n\")\n",
        "    \n",
        "    # 调用RAG Agent\n",
        "    result = rag_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
        "    })\n",
        "    \n",
        "    # 获取答案\n",
        "    answer = result['messages'][-1].content\n",
        "    print(f\"回答: {answer}\\n\")\n",
        "    \n",
        "    # 查看是否调用了检索工具\n",
        "    tool_messages = [msg for msg in result['messages'] if hasattr(msg, 'type') and msg.type == 'tool']\n",
        "    if tool_messages:\n",
        "        print(f\"✓ 使用了检索工具\")\n",
        "        # 获取检索到的文档（从artifact）\n",
        "        for tool_msg in tool_messages:\n",
        "            if hasattr(tool_msg, 'artifact') and tool_msg.artifact:\n",
        "                print(f\"  检索了 {len(tool_msg.artifact)} 个文档片段\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "    print()\n",
        "\n",
        "print(\"💡 观察要点：\")\n",
        "print(\"  ✓ Agent自动决定何时使用检索工具\")\n",
        "print(\"  ✓ 答案基于检索到的文档内容\")\n",
        "print(\"  ✓ 可以通过artifact获取原始文档\")\n",
        "print(\"  ✓ 所有执行过程都被LangSmith追踪\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤5：流式输出\n",
        "\n",
        "实现实时流式输出，提升用户体验。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤5：流式输出】\n",
            "\n",
            "问题: 请详细介绍LangChain的RAG技术\n",
            "\n",
            "回答（流式）: 【文档片段 1】\n",
            "# LangChain 学习指南\n",
            "\n",
            "## 什么是LangChain？\n",
            "\n",
            "LangChain是一个强大的开源框架，专门用于构建基于大语言模型（LLM）的应用程序。它提供了一套完整的工具和抽象，使开发者能够轻松地将LLM集成到各种应用场景中。\n",
            "\n",
            "## 核心组件\n",
            "\n",
            "### 1. Models（模型）\n",
            "LangChain支持多种LLM提供商，包括OpenAI、Anthropic、Google等。通过统一的接口，开发者可以轻松切换不同的模型。\n",
            "\n",
            "### 2. Prompts（提示词）\n",
            "提供了Prompt模板和管理工具，帮助开发者更好地设计和优化提示词。\n",
            "\n",
            "### 3. Chains（链）\n",
            "将多个组件串联起来，形成复杂的处理流程。\n",
            "\n",
            "### 4. Agents（代理）\n",
            "Agent可以根据用户输入动态决定调用哪些工具，实现更智能的交互。\n",
            "\n",
            "### 5. Memory（记忆）\n",
            "为对话系统提供上下文记忆能力，使对话更加连贯。\n",
            "\n",
            "## RAG技术\n",
            "\n",
            "检索增强生成（RAG）是LangChain的核心应用之一。它通过以下步骤工作：\n",
            "\n",
            "【文档片段 2】\n",
            "检索增强生成（RAG）是LangChain的核心应用之一。它通过以下步骤工作：\n",
            "\n",
            "1. 文档加载：使用Document Loaders读取各种格式的文档\n",
            "2. 文本分割：将长文档切分成小块\n",
            "3. 向量化：将文本转换为向量表示\n",
            "4. 存储：将向量存储在向量数据库中\n",
            "5. 检索：根据用户问题检索相关文档\n",
            "6. 生成：基于检索到的内容生成答案\n",
            "\n",
            "## 使用场景\n",
            "\n",
            "- 智能客服系统\n",
            "- 文档问答系统\n",
            "- 代码助手\n",
            "- 内容生成工具\n",
            "- 数据分析助手\n",
            "\n",
            "## 最佳实践\n",
            "\n",
            "1. 合理设计Prompt模板\n",
            "2. 选择合适的文本分割策略\n",
            "3. 优化向量检索参数\n",
            "4. 使用评估工具持续改进\n",
            "5. 注意成本和性能平衡LangChain的RAG（检索增强生成）技术是一种结合文档检索与生成模型的技术。其工作流程包括：首先使用Document Loaders加载各种格式的文档，然后将长文档切分成小块，接着将文本向量化并存储到向量数据库中。用户提问时，系统会检索相关文档，再基于检索结果生成答案。RAG技术广泛应用于智能客服、文档问答、代码助手等场景，且在使用时需合理设计Prompt、优化文本分割和向量检索参数以提升效果。\n",
            "\n",
            "✓ 流式输出完成\n",
            "\n",
            "💡 流式输出的优势：\n",
            "  - 用户可以立即看到响应\n",
            "  - 提升用户体验\n",
            "  - 适合长文本生成\n",
            "  - 可以提前中断不需要的输出\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤5：流式输出】\\n\")\n",
        "\n",
        "import time\n",
        "\n",
        "question = \"请详细介绍LangChain的RAG技术\"\n",
        "print(f\"问题: {question}\\n\")\n",
        "print(\"回答（流式）: \", end=\"\", flush=True)\n",
        "\n",
        "# 使用stream模式\n",
        "for chunk in rag_agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
        "    stream_mode=\"messages\"\n",
        "):\n",
        "    # 只输出AI的内容\n",
        "    if isinstance(chunk, tuple) and len(chunk) == 2:\n",
        "        message, metadata = chunk\n",
        "        if hasattr(message, 'content') and message.content:\n",
        "            print(message.content, end=\"\", flush=True)\n",
        "            time.sleep(0.02)  # 模拟打字效果\n",
        "\n",
        "print(\"\\n\\n✓ 流式输出完成\")\n",
        "print(\"\\n💡 流式输出的优势：\")\n",
        "print(\"  - 用户可以立即看到响应\")\n",
        "print(\"  - 提升用户体验\")\n",
        "print(\"  - 适合长文本生成\")\n",
        "print(\"  - 可以提前中断不需要的输出\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 步骤6：系统评估\n",
        "\n",
        "评估RAG系统的性能和质量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【步骤6：系统评估】\n",
            "\n",
            "=== 运行评估 ===\n",
            "\n",
            "评估样本 1/3: 什么是LangChain？...\n",
            "  关键词覆盖率: 100.00%\n",
            "  找到关键词: ['框架', 'LLM', '应用']\n",
            "\n",
            "评估样本 2/3: LangChain的核心组件有哪些？...\n",
            "  关键词覆盖率: 100.00%\n",
            "  找到关键词: ['Models', 'Prompts', 'Chains', 'Agents']\n",
            "\n",
            "评估样本 3/3: RAG技术包含哪些步骤？...\n",
            "  关键词覆盖率: 100.00%\n",
            "  找到关键词: ['文档加载', '分割', '向量化', '检索', '生成']\n",
            "\n",
            "=== 评估汇总 ===\n",
            "\n",
            "平均关键词覆盖率: 100.00%\n",
            "测试样本数: 3\n",
            "\n",
            "💡 评估要点：\n",
            "  ✓ 多维度评估：关键词、相关性、忠实度\n",
            "  ✓ 使用测试集持续监控质量\n",
            "  ✓ 结合LangSmith进行在线评估\n",
            "  ✓ 根据评估结果优化系统\n"
          ]
        }
      ],
      "source": [
        "print(\"【步骤6：系统评估】\\n\")\n",
        "\n",
        "# 创建评估数据集\n",
        "eval_dataset = [\n",
        "    {\n",
        "        \"question\": \"什么是LangChain？\",\n",
        "        \"expected_keywords\": [\"框架\", \"LLM\", \"应用\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"LangChain的核心组件有哪些？\",\n",
        "        \"expected_keywords\": [\"Models\", \"Prompts\", \"Chains\", \"Agents\"]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"RAG技术包含哪些步骤？\",\n",
        "        \"expected_keywords\": [\"文档加载\", \"分割\", \"向量化\", \"检索\", \"生成\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"=== 运行评估 ===\\n\")\n",
        "\n",
        "# 定义评估器\n",
        "def keyword_coverage_evaluator(question, answer, expected_keywords):\n",
        "    \"\"\"评估答案是否包含关键词\"\"\"\n",
        "    answer_lower = answer.lower()\n",
        "    found = [kw for kw in expected_keywords if kw.lower() in answer_lower]\n",
        "    score = len(found) / len(expected_keywords)\n",
        "    return score, found\n",
        "\n",
        "# 评估每个问题\n",
        "results = []\n",
        "for i, test_case in enumerate(eval_dataset, 1):\n",
        "    print(f\"评估样本 {i}/{len(eval_dataset)}: {test_case['question'][:30]}...\")\n",
        "    \n",
        "    # 运行RAG系统\n",
        "    result = rag_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": test_case['question']}]\n",
        "    })\n",
        "    answer = result['messages'][-1].content\n",
        "    \n",
        "    # 评估\n",
        "    score, found_keywords = keyword_coverage_evaluator(\n",
        "        test_case['question'],\n",
        "        answer,\n",
        "        test_case['expected_keywords']\n",
        "    )\n",
        "    \n",
        "    results.append({\n",
        "        \"question\": test_case['question'],\n",
        "        \"answer\": answer[:80] + \"...\",\n",
        "        \"score\": score,\n",
        "        \"found_keywords\": found_keywords\n",
        "    })\n",
        "    \n",
        "    print(f\"  关键词覆盖率: {score:.2%}\")\n",
        "    print(f\"  找到关键词: {found_keywords}\")\n",
        "    print()\n",
        "\n",
        "# 汇总结果\n",
        "avg_score = sum(r['score'] for r in results) / len(results)\n",
        "\n",
        "print(\"=== 评估汇总 ===\\n\")\n",
        "print(f\"平均关键词覆盖率: {avg_score:.2%}\")\n",
        "print(f\"测试样本数: {len(results)}\")\n",
        "print()\n",
        "\n",
        "print(\"💡 评估要点：\")\n",
        "print(\"  ✓ 多维度评估：关键词、相关性、忠实度\")\n",
        "print(\"  ✓ 使用测试集持续监控质量\")\n",
        "print(\"  ✓ 结合LangSmith进行在线评估\")\n",
        "print(\"  ✓ 根据评估结果优化系统\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 项目总结\n",
        "\n",
        "### 🎉 完成的功能\n",
        "\n",
        "本项目实现了一个完整的RAG智能问答系统：\n",
        "\n",
        "| 功能模块 | 实现方式 | 关键技术 |\n",
        "|---------|---------|---------|\n",
        "| **文档处理** | TextLoader + TextSplitter | 加载和分割文档 |\n",
        "| **向量检索** | OpenAIEmbeddings + InMemoryVectorStore | 语义检索 |\n",
        "| **智能问答** | RAG Agent + retrieve_documents工具 | 动态检索和生成 |\n",
        "| **引用来源** | response_format=\"content_and_artifact\" | 返回原始文档 |\n",
        "| **流式输出** | stream_mode=\"messages\" | 实时响应 |\n",
        "| **质量评估** | 关键词覆盖率评估器 | 自动化评估 |\n",
        "\n",
        "### 🔑 关键技术点\n",
        "\n",
        "#### 1. 文档处理流程\n",
        "\n",
        "```python\n",
        "文档 → TextLoader → 加载\n",
        "     → RecursiveCharacterTextSplitter → 分割\n",
        "     → OpenAIEmbeddings → 向量化\n",
        "     → InMemoryVectorStore → 存储\n",
        "```\n",
        "\n",
        "#### 2. RAG Agent设计\n",
        "\n",
        "```python\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_documents(query: str):\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return formatted_content, docs  # 内容 + 原始文档\n",
        "\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[retrieve_documents],\n",
        "    system_prompt=\"基于文档回答问题...\"\n",
        ")\n",
        "```\n",
        "\n",
        "#### 3. 检索参数优化\n",
        "\n",
        "| 参数 | 作用 | 推荐值 |\n",
        "|------|------|--------|\n",
        "| `chunk_size` | chunk大小 | 500-1000 |\n",
        "| `chunk_overlap` | 重叠大小 | 50-100 |\n",
        "| `k` | 检索数量 | 3-5 |\n",
        "| `temperature` | 生成随机性 | 0-0.3（RAG场景） |\n",
        "\n",
        "### 💡 优化建议\n",
        "\n",
        "#### 性能优化\n",
        "\n",
        "1. **检索优化**\n",
        "   - 调整chunk_size平衡上下文和精度\n",
        "   - 增加chunk_overlap保持语义连贯\n",
        "   - 使用MMR（最大边际相关性）避免重复\n",
        "\n",
        "2. **生成优化**\n",
        "   - 设置temperature=0提高确定性\n",
        "   - 优化system_prompt引导答案格式\n",
        "   - 限制答案长度避免冗余\n",
        "\n",
        "3. **成本优化**\n",
        "   - 使用较小的embedding模型\n",
        "   - 减少检索的k值\n",
        "   - 使用缓存避免重复计算\n",
        "\n",
        "#### 质量优化\n",
        "\n",
        "1. **提高准确性**\n",
        "   - 优化文档分割策略\n",
        "   - 增加检索的文档数量\n",
        "   - 使用更强的LLM模型\n",
        "\n",
        "2. **提高相关性**\n",
        "   - 改进检索算法（混合检索）\n",
        "   - 过滤低质量文档\n",
        "   - 重排序检索结果\n",
        "\n",
        "3. **提高忠实度**\n",
        "   - 强调\"基于文档回答\"的提示\n",
        "   - 添加引用来源\n",
        "   - 使用评估器检测幻觉\n",
        "\n",
        "### 🚀 扩展方向\n",
        "\n",
        "1. **多文档支持** - 支持PDF、Word、网页等\n",
        "2. **混合检索** - 结合关键词和语义检索\n",
        "3. **多轮对话** - 添加Checkpointer支持上下文\n",
        "4. **答案评分** - 返回置信度分数\n",
        "5. **引用展示** - 在答案中标注来源\n",
        "6. **用户反馈** - 收集反馈持续优化\n",
        "\n",
        "### 📊 性能指标\n",
        "\n",
        "评估RAG系统的关键指标：\n",
        "\n",
        "| 指标 | 说明 | 目标值 |\n",
        "|------|------|--------|\n",
        "| **检索准确率** | 检索到的文档是否相关 | > 80% |\n",
        "| **答案相关性** | 答案是否回答了问题 | > 90% |\n",
        "| **答案忠实度** | 答案是否基于文档 | > 95% |\n",
        "| **响应延迟** | 从问题到答案的时间 | < 3秒 |\n",
        "| **Token成本** | 每次查询的token使用 | 优化 |\n",
        "\n",
        "---\n",
        "\n",
        "## 关键收获\n",
        "\n",
        "✅ **完整的RAG流程** - 从文档到答案的端到端实现\n",
        "\n",
        "✅ **Agent vs Chain** - Agent更灵活，可以动态决定是否检索\n",
        "\n",
        "✅ **response_format=\"content_and_artifact\"** - 同时返回内容和原始文档\n",
        "\n",
        "✅ **流式输出** - 提升用户体验的关键\n",
        "\n",
        "✅ **评估驱动优化** - 基于数据持续改进\n",
        "\n",
        "✅ **LangSmith集成** - 所有操作自动追踪，便于调试\n",
        "\n",
        "✅ **模块化设计** - 每个组件可独立替换和优化\n",
        "\n",
        "---\n",
        "\n",
        "## 下一步\n",
        "\n",
        "完成智能问答系统后，可以：\n",
        "\n",
        "1. **优化系统** - 尝试不同的分割策略、检索参数\n",
        "2. **添加功能** - 多轮对话、引用展示、答案评分\n",
        "3. **第24章** - 构建对话机器人项目\n",
        "4. **部署上线** - 将系统部署到生产环境\n",
        "\n",
        "🎉 恭喜！你已经完成了第一个完整的RAG项目！\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
