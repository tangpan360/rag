{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第06章：Document Processing（文档处理）\n",
        "\n",
        "## 学习目标\n",
        "\n",
        "本章将学习：\n",
        "1. Document对象的结构和作用\n",
        "2. 使用Document Loaders加载各种格式的文档\n",
        "3. 使用Text Splitters将文档分割成小块\n",
        "4. 理解chunk size和chunk overlap的作用\n",
        "5. 掌握RecursiveCharacterTextSplitter的使用\n",
        "\n",
        "## 为什么需要Document Processing？\n",
        "\n",
        "在构建RAG系统时，我们需要：\n",
        "- 从各种来源加载文档（PDF、网页、数据库等）\n",
        "- 将大文档分割成小块，以便：\n",
        "  - 适应模型的上下文窗口限制\n",
        "  - 提高检索的精确度\n",
        "  - 避免无关信息干扰\n",
        "\n",
        "Document Processing是RAG流程的第一步，也是基础。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "环境配置完成\n",
            "模型: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# 环境配置\n",
        "import os\n",
        "import sys\n",
        "\n",
        "_project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "sys.path.append(_project_root)\n",
        "\n",
        "from config import config\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "from langchain.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# 初始化模型\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4.1-mini\",\n",
        "    temperature=0,  # 工具调用建议使用低温度\n",
        "    api_key=config.CLOUD_API_KEY,\n",
        "    base_url=config.CLOUD_BASE_URL,\n",
        ")\n",
        "\n",
        "print(\"环境配置完成\")\n",
        "print(f\"模型: {model.model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document对象\n",
        "\n",
        "Document是LangChain中表示文档的核心抽象。\n",
        "\n",
        "### Document的结构\n",
        "\n",
        "一个Document对象包含三个主要属性：\n",
        "- `page_content`：文档的文本内容（字符串）\n",
        "- `metadata`：文档的元数据（字典），可以包含来源、页码、作者等信息\n",
        "- `id`：可选的文档标识符（字符串）\n",
        "\n",
        "Document对象通常代表一个大文档的一个片段（chunk）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【创建Document对象】\n",
            "\n",
            "文档 1:\n",
            "  内容: 狗是很好的伙伴，以忠诚和友好而闻名。\n",
            "  元数据: {'source': '宠物文档', 'category': '哺乳动物'}\n",
            "  ID: None\n",
            "\n",
            "文档 2:\n",
            "  内容: 猫是独立的宠物，经常喜欢自己的空间。\n",
            "  元数据: {'source': '宠物文档', 'category': '哺乳动物'}\n",
            "  ID: None\n",
            "\n",
            "文档 3:\n",
            "  内容: Python是一种高级编程语言，以其简洁的语法而闻名。\n",
            "  元数据: {'source': '编程文档', 'category': '技术', 'difficulty': 'beginner'}\n",
            "  ID: None\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "print(\"【创建Document对象】\")\n",
        "print()\n",
        "\n",
        "# 创建示例文档\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"狗是很好的伙伴，以忠诚和友好而闻名。\",\n",
        "        metadata={\"source\": \"宠物文档\", \"category\": \"哺乳动物\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"猫是独立的宠物，经常喜欢自己的空间。\",\n",
        "        metadata={\"source\": \"宠物文档\", \"category\": \"哺乳动物\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Python是一种高级编程语言，以其简洁的语法而闻名。\",\n",
        "        metadata={\"source\": \"编程文档\", \"category\": \"技术\", \"difficulty\": \"beginner\"}\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 查看Document对象\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"文档 {i}:\")\n",
        "    print(f\"  内容: {doc.page_content}\")\n",
        "    print(f\"  元数据: {doc.metadata}\")\n",
        "    print(f\"  ID: {doc.id}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Document Loaders（文档加载器）\n",
        "\n",
        "Document Loaders用于从各种数据源加载文档，并将其转换为统一的Document对象。\n",
        "\n",
        "### 常见的Document Loaders\n",
        "\n",
        "LangChain提供了数百种Document Loaders，支持：\n",
        "- 文件格式：PDF、CSV、JSON、TXT、DOCX、EPUB、PPTX等\n",
        "- 数据源：Google Drive、Slack、Notion、数据库等\n",
        "- 网络内容：网页、API等\n",
        "\n",
        "### Document Loader的标准接口\n",
        "\n",
        "所有Document Loaders都实现统一的接口：\n",
        "- `load()`：一次性加载所有文档\n",
        "- `load_and_split()`：加载并自动分割文档\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例1：加载文本文件】\n",
            "\n",
            "加载了 1 个文档\n",
            "\n",
            "文档内容:\n",
            "  page_content: LangChain是一个用于构建基于语言模型应用的框架。\n",
            "它提供了强大的工具来处理文档、构建RAG系统和创建Agent。\n",
            "LangChain支持多种语言模型提供商，包括OpenAI、Anthropic等。\n",
            "\n",
            "  metadata: {'source': '../data/sample.txt'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 3.1 加载文本文件\n",
        "\n",
        "print(\"【示例1：加载文本文件】\")\n",
        "print()\n",
        "\n",
        "# 创建一个示例文本文件\n",
        "with open(\"../data/sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"LangChain是一个用于构建基于语言模型应用的框架。\\n\")\n",
        "    f.write(\"它提供了强大的工具来处理文档、构建RAG系统和创建Agent。\\n\")\n",
        "    f.write(\"LangChain支持多种语言模型提供商，包括OpenAI、Anthropic等。\\n\")\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# 加载文本文件\n",
        "loader = TextLoader(\"../data/sample.txt\", encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"加载了 {len(docs)} 个文档\")\n",
        "print()\n",
        "print(\"文档内容:\")\n",
        "print(f\"  page_content: {docs[0].page_content}\")\n",
        "print(f\"  metadata: {docs[0].metadata}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例2：加载CSV文件】\n",
            "\n",
            "加载了 3 个文档\n",
            "\n",
            "文档 1:\n",
            "  内容: 产品名称: 笔记本电脑\n",
            "价格: 5999\n",
            "描述: 高性能办公笔记本...\n",
            "  元数据: {'source': '../data/products.csv', 'row': 0}\n",
            "\n",
            "文档 2:\n",
            "  内容: 产品名称: 鼠标\n",
            "价格: 99\n",
            "描述: 无线蓝牙鼠标...\n",
            "  元数据: {'source': '../data/products.csv', 'row': 1}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 3.2 加载CSV文件\n",
        "\n",
        "print(\"【示例2：加载CSV文件】\")\n",
        "print()\n",
        "\n",
        "# 创建示例CSV文件\n",
        "import csv\n",
        "\n",
        "with open(\"../data/products.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"产品名称\", \"价格\", \"描述\"])\n",
        "    writer.writerow([\"笔记本电脑\", \"5999\", \"高性能办公笔记本\"])\n",
        "    writer.writerow([\"鼠标\", \"99\", \"无线蓝牙鼠标\"])\n",
        "    writer.writerow([\"键盘\", \"299\", \"机械键盘\"])\n",
        "\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# 加载CSV文件\n",
        "loader = CSVLoader(\"../data/products.csv\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"加载了 {len(docs)} 个文档\")\n",
        "print()\n",
        "for i, doc in enumerate(docs[:2], 1):\n",
        "    print(f\"文档 {i}:\")\n",
        "    print(f\"  内容: {doc.page_content[:100]}...\")\n",
        "    print(f\"  元数据: {doc.metadata}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例3：加载JSON文件】\n",
            "\n",
            "加载了 3 个文档\n",
            "\n",
            "文档 1:\n",
            "  内容: {\n",
            "  \"name\": \"张三\",\n",
            "  \"age\": 25,\n",
            "  \"city\": \"北京\"\n",
            "}\n",
            "  元数据: {'source': '/home/iip/tp/038-project/rag/data/users.json', 'seq_num': 1}\n",
            "\n",
            "文档 2:\n",
            "  内容: {\n",
            "  \"name\": \"李四\",\n",
            "  \"age\": 30,\n",
            "  \"city\": \"上海\"\n",
            "}\n",
            "  元数据: {'source': '/home/iip/tp/038-project/rag/data/users.json', 'seq_num': 2}\n",
            "\n",
            "文档 3:\n",
            "  内容: {\n",
            "  \"name\": \"王五\",\n",
            "  \"age\": 28,\n",
            "  \"city\": \"深圳\"\n",
            "}\n",
            "  元数据: {'source': '/home/iip/tp/038-project/rag/data/users.json', 'seq_num': 3}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 3.3 加载JSON文件\n",
        "\n",
        "print(\"【示例3：加载JSON文件】\")\n",
        "print()\n",
        "\n",
        "# 创建示例JSON文件\n",
        "import json\n",
        "\n",
        "data = {\n",
        "    \"users\": [\n",
        "        {\"name\": \"张三\", \"age\": 25, \"city\": \"北京\"},\n",
        "        {\"name\": \"李四\", \"age\": 30, \"city\": \"上海\"},\n",
        "        {\"name\": \"王五\", \"age\": 28, \"city\": \"深圳\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(\"../data/users.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "\n",
        "# 加载JSON文件（提取users数组中的每个对象）\n",
        "loader = JSONLoader(\n",
        "    file_path=\"../data/users.json\",\n",
        "    jq_schema=\".users[]\",\n",
        "    text_content=False\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"加载了 {len(docs)} 个文档\")\n",
        "print()\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"文档 {i}:\")\n",
        "    # 将转义的字符串重新解码\n",
        "    content_obj = json.loads(doc.page_content)\n",
        "    print(f\"  内容: {json.dumps(content_obj, ensure_ascii=False, indent=2)}\")\n",
        "    print(f\"  元数据: {doc.metadata}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Splitters（文本分割器）\n",
        "\n",
        "### 为什么需要Text Splitters？\n",
        "\n",
        "当我们加载文档后，通常需要将其分割成更小的块，原因包括：\n",
        "\n",
        "1. **上下文窗口限制**：大多数模型都有上下文窗口限制（如4K、8K tokens）\n",
        "2. **检索精确度**：小块更容易匹配具体的查询\n",
        "3. **成本优化**：只传递相关部分给模型，减少token消耗\n",
        "4. **语义完整性**：避免无关信息干扰模型理解\n",
        "\n",
        "### 分割策略\n",
        "\n",
        "LangChain提供多种分割策略：\n",
        "\n",
        "1. **基于长度的分割**：\n",
        "   - CharacterTextSplitter：按字符数分割\n",
        "   - TokenTextSplitter：按token数分割\n",
        "\n",
        "2. **基于文本结构的分割**（推荐）：\n",
        "   - RecursiveCharacterTextSplitter：递归使用多种分隔符\n",
        "   - 保持段落、句子的完整性\n",
        "\n",
        "3. **基于文档结构的分割**：\n",
        "   - MarkdownTextSplitter：按Markdown标题分割\n",
        "   - HTMLTextSplitter：按HTML标签分割\n",
        "   - CodeTextSplitter：按代码结构分割\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 208, which is longer than the specified 100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例1：CharacterTextSplitter】\n",
            "\n",
            "文本被分割成 5 个块\n",
            "\n",
            "块 1 (90 字符):\n",
            "LangChain 核心功能详解\n",
            "\n",
            "LangChain 是一个用于构建大语言模型（LLM）应用的开源框架，它提供了丰富的工具和组件。\n",
            "\n",
            "复我也是真的非常的醉了，可惜了，真的很不错。\n",
            "------------------------------------------------------------\n",
            "\n",
            "块 2 (58 字符):\n",
            "杂非常复杂真是复杂极了非常开心\n",
            "\n",
            "的\n",
            "\n",
            "AI\n",
            "\n",
            "应\n",
            "\n",
            "用\n",
            "\n",
            "。\n",
            "\n",
            "核\n",
            "\n",
            "心\n",
            "\n",
            "模\n",
            "\n",
            "块\n",
            "\n",
            "包\n",
            "\n",
            "括\n",
            "\n",
            "：\n",
            "\n",
            "这\n",
            "\n",
            "些\n",
            "------------------------------------------------------------\n",
            "\n",
            "块 3 (94 字符):\n",
            "模\n",
            "\n",
            "块\n",
            "\n",
            "包\n",
            "\n",
            "括\n",
            "\n",
            "：\n",
            "\n",
            "这\n",
            "\n",
            "些\n",
            "\n",
            "1. 模型集成（Model I/O）：支持对接OpenAI、Anthropic、百度文心等主流LLM，提供统一的调用接口，简化模型切换和参数管理。\n",
            "------------------------------------------------------------\n",
            "\n",
            "块 4 (208 字符):\n",
            "2. 数据连接（Data Connection）：提供文档加载、文本分割、向量存储等功能，支持RAG（检索增强生成）场景，让LLM能结合外部数据回答问题。\n",
            "3. 链（Chains）：将多个组件组合成流水线，比如“加载文档→分割文本→向量检索→生成回答”，实现端到端的AI应用流程。\n",
            "4. 智能代理（Agents）：让LLM能自主决策调用哪些工具（如计算器、搜索引擎），完成复杂任务，比如“分析财报并生成可视化图表”。\n",
            "------------------------------------------------------------\n",
            "\n",
            "块 5 (158 字符):\n",
            "LangChain的优势在于灵活性和扩展性：开发者可以按需组合组件，也可以自定义组件适配特定场景；同时它支持多种部署方式，包括本地部署、云部署、边缘部署等，满足不同的生产环境需求。\n",
            "在实际应用中，LangChain常被用于构建智能客服、文档问答系统、代码生成工具、数据分析助手等，是当前LLM应用开发的主流框架之一。\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 4.1 CharacterTextSplitter（基础分割器）\n",
        "\n",
        "print(\"【示例1：CharacterTextSplitter】\")\n",
        "print()\n",
        "\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 创建一个较长的文本\n",
        "text = \"\"\"\n",
        "LangChain 核心功能详解\n",
        "\n",
        "LangChain 是一个用于构建大语言模型（LLM）应用的开源框架，它提供了丰富的工具和组件。\n",
        "\n",
        "复我也是真的非常的醉了，可惜了，真的很不错。\n",
        "\n",
        "杂非常复杂真是复杂极了非常开心\n",
        "\n",
        "的\n",
        "\n",
        "AI\n",
        "\n",
        "应\n",
        "\n",
        "用\n",
        "\n",
        "。\n",
        "\n",
        "核\n",
        "\n",
        "心\n",
        "\n",
        "模\n",
        "\n",
        "块\n",
        "\n",
        "包\n",
        "\n",
        "括\n",
        "\n",
        "：\n",
        "\n",
        "这\n",
        "\n",
        "些\n",
        "\n",
        "1. 模型集成（Model I/O）：支持对接OpenAI、Anthropic、百度文心等主流LLM，提供统一的调用接口，简化模型切换和参数管理。\n",
        "\n",
        "2. 数据连接（Data Connection）：提供文档加载、文本分割、向量存储等功能，支持RAG（检索增强生成）场景，让LLM能结合外部数据回答问题。\n",
        "3. 链（Chains）：将多个组件组合成流水线，比如“加载文档→分割文本→向量检索→生成回答”，实现端到端的AI应用流程。\n",
        "4. 智能代理（Agents）：让LLM能自主决策调用哪些工具（如计算器、搜索引擎），完成复杂任务，比如“分析财报并生成可视化图表”。\n",
        "\n",
        "LangChain的优势在于灵活性和扩展性：开发者可以按需组合组件，也可以自定义组件适配特定场景；同时它支持多种部署方式，包括本地部署、云部署、边缘部署等，满足不同的生产环境需求。\n",
        "在实际应用中，LangChain常被用于构建智能客服、文档问答系统、代码生成工具、数据分析助手等，是当前LLM应用开发的主流框架之一。\n",
        "\"\"\"\n",
        "\n",
        "# 创建分割器\n",
        "splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",  # 使用双换行符作为分隔符\n",
        "    chunk_size=100,    # 每块最大100字符\n",
        "    chunk_overlap=20,  # 块之间重叠20字符\n",
        ")\n",
        "\n",
        "# 分割文本\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(f\"文本被分割成 {len(chunks)} 个块\")\n",
        "print()\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"块 {i} ({len(chunk)} 字符):\")\n",
        "    print(f\"{chunk.strip()}\")\n",
        "    print(\"-\" * 60)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 RecursiveCharacterTextSplitter（推荐）\n",
        "\n",
        "RecursiveCharacterTextSplitter是官方推荐的通用文本分割器。\n",
        "\n",
        "#### 工作原理\n",
        "\n",
        "它会递归地尝试使用以下分隔符（按顺序）：\n",
        "1. `\\n\\n`（双换行，段落）\n",
        "2. `\\n`（单换行，行）\n",
        "3. ` `（空格，单词）\n",
        "4. `\"\"`（字符）\n",
        "\n",
        "这样可以尽可能保持文本结构的完整性。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例2：RecursiveCharacterTextSplitter】\n",
            "\n",
            "文本被分割成 3 个块\n",
            "\n",
            "块 1 (128 字符):\n",
            "# LangChain学习指南\n",
            "\n",
            "## 什么是LangChain？\n",
            "\n",
            "LangChain是一个用于构建基于语言模型应用的框架。它提供了一系列工具和抽象，使开发者能够轻松地：\n",
            "- 连接多个组件\n",
            "- 管理提示词\n",
            "- 调用语言模型\n",
            "- 处理输出\n",
            "\n",
            "## 核心概念\n",
            "============================================================\n",
            "\n",
            "块 2 (112 字符):\n",
            "## 核心概念\n",
            "\n",
            "### Models\n",
            "Models是LangChain的基础。LangChain支持各种语言模型提供商。\n",
            "\n",
            "### Prompts\n",
            "Prompts用于指导模型的行为。好的prompt可以显著提高模型的性能。\n",
            "============================================================\n",
            "\n",
            "块 3 (39 字符):\n",
            "### Chains\n",
            "Chains允许你将多个组件组合在一起，创建复杂的应用。\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"【示例2：RecursiveCharacterTextSplitter】\")\n",
        "print()\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 创建一个包含多种结构的文本\n",
        "text = \"\"\"# LangChain学习指南\n",
        "\n",
        "## 什么是LangChain？\n",
        "\n",
        "LangChain是一个用于构建基于语言模型应用的框架。它提供了一系列工具和抽象，使开发者能够轻松地：\n",
        "- 连接多个组件\n",
        "- 管理提示词\n",
        "- 调用语言模型\n",
        "- 处理输出\n",
        "\n",
        "## 核心概念\n",
        "\n",
        "### Models\n",
        "Models是LangChain的基础。LangChain支持各种语言模型提供商。\n",
        "\n",
        "### Prompts\n",
        "Prompts用于指导模型的行为。好的prompt可以显著提高模型的性能。\n",
        "\n",
        "### Chains\n",
        "Chains允许你将多个组件组合在一起，创建复杂的应用。\n",
        "\"\"\"\n",
        "\n",
        "# 创建递归分割器\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,     # 每块最大150字符\n",
        "    chunk_overlap=30,   # 块之间重叠30字符\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "# 分割文本\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(f\"文本被分割成 {len(chunks)} 个块\")\n",
        "print()\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"块 {i} ({len(chunk)} 字符):\")\n",
        "    print(chunk.strip())\n",
        "    print(\"=\" * 60)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【示例3：分割Document对象】\n",
            "\n",
            "原始文档被分割成 3 个子文档\n",
            "\n",
            "子文档 1:\n",
            "  内容: LangChain是一个强大的框架，用于构建基于语言模型的应用。它提供了丰富的工具和抽象层。\n",
            "  元数据: {'source': 'LangChain教程', 'chapter': 1, 'start_index': 1}\n",
            "\n",
            "子文档 2:\n",
            "  内容: RAG（检索增强生成）是LangChain的核心应用场景之一。通过RAG，我们可以让模型访问外部知识库。\n",
            "  元数据: {'source': 'LangChain教程', 'chapter': 1, 'start_index': 49}\n",
            "\n",
            "子文档 3:\n",
            "  内容: Agent是LangChain的另一个重要特性。Agent能够自主决定使用哪些工具来完成任务。\n",
            "\n",
            "LangGraph则是用于构建复杂多智能体系统的工具，它基于状态机的概念。\n",
            "  元数据: {'source': 'LangChain教程', 'chapter': 1, 'start_index': 103}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 4.3 分割Document对象\n",
        "\n",
        "print(\"【示例3：分割Document对象】\")\n",
        "print()\n",
        "\n",
        "# 创建一个长文档\n",
        "long_doc = Document(\n",
        "    page_content=\"\"\"\n",
        "LangChain是一个强大的框架，用于构建基于语言模型的应用。它提供了丰富的工具和抽象层。\n",
        "\n",
        "RAG（检索增强生成）是LangChain的核心应用场景之一。通过RAG，我们可以让模型访问外部知识库。\n",
        "\n",
        "Agent是LangChain的另一个重要特性。Agent能够自主决定使用哪些工具来完成任务。\n",
        "\n",
        "LangGraph则是用于构建复杂多智能体系统的工具，它基于状态机的概念。\n",
        "    \"\"\",\n",
        "    metadata={\"source\": \"LangChain教程\", \"chapter\": 1}\n",
        ")\n",
        "\n",
        "# 使用split_documents方法\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    add_start_index=True,  # 记录每个块在原文中的起始位置\n",
        ")\n",
        "\n",
        "# 分割文档\n",
        "split_docs = text_splitter.split_documents([long_doc])\n",
        "\n",
        "print(f\"原始文档被分割成 {len(split_docs)} 个子文档\")\n",
        "print()\n",
        "for i, doc in enumerate(split_docs, 1):\n",
        "    print(f\"子文档 {i}:\")\n",
        "    print(f\"  内容: {doc.page_content.strip()}\")\n",
        "    print(f\"  元数据: {doc.metadata}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 关键参数详解\n",
        "\n",
        "### 5.1 chunk_size（块大小）\n",
        "\n",
        "chunk_size决定每个文本块的最大长度。\n",
        "\n",
        "#### 如何选择chunk_size？\n",
        "\n",
        "| chunk_size | 适用场景 | 优点 | 缺点 |\n",
        "|-----------|---------|------|------|\n",
        "| 小（100-300） | 精确匹配、问答 | 检索精确 | 可能丢失上下文 |\n",
        "| 中（500-1000） | 通用RAG（推荐） | 平衡性好 | - |\n",
        "| 大（1500-2000） | 需要完整上下文 | 上下文完整 | 检索不够精确 |\n",
        "\n",
        "#### 影响因素\n",
        "\n",
        "1. **模型上下文窗口**：不能超过模型限制\n",
        "2. **内容类型**：代码、文章、对话等需要不同的大小\n",
        "3. **检索需求**：需要精确匹配还是完整上下文\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 chunk_overlap（块重叠）\n",
        "\n",
        "chunk_overlap决定相邻文本块之间的重叠字符数。\n",
        "\n",
        "#### 为什么需要重叠？\n",
        "\n",
        "重叠可以解决以下问题：\n",
        "1. **避免语句被截断**：重要信息可能横跨两个块的边界\n",
        "2. **保持上下文连贯性**：让每个块都有前后文\n",
        "3. **提高检索召回率**：同一信息可能出现在多个块中\n",
        "\n",
        "#### 如何选择chunk_overlap？\n",
        "\n",
        "一般建议：\n",
        "- chunk_overlap = chunk_size * 10%-20%\n",
        "- 例如：chunk_size=1000，则chunk_overlap=100-200\n",
        "\n",
        "过大的重叠会导致：\n",
        "- 存储空间浪费\n",
        "- 检索时重复内容过多\n",
        "\n",
        "过小的重叠会导致：\n",
        "- 重要信息被截断\n",
        "- 上下文不完整\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【对比：有无重叠的区别】\n",
            "\n",
            "无重叠:\n",
            "  块1: 人工智能正在改变世界。机器学习是人工智能的核心技术。深度学习\n",
            "  块2: 推动了AI的快速发展。\n",
            "\n",
            "有重叠（overlap=10）:\n",
            "  块1: 人工智能正在改变世界。机器学习是人工智能的核心技术。深度学习\n",
            "  块2: 的核心技术。深度学习推动了AI的快速发展。\n",
            "\n",
            "注意：\n",
            "- 无重叠：每个块之间完全独立，可能导致信息被截断\n",
            "- 有重叠：块之间有共同部分，保证了上下文的连续性\n"
          ]
        }
      ],
      "source": [
        "print(\"【对比：有无重叠的区别】\")\n",
        "print()\n",
        "\n",
        "text = \"人工智能正在改变世界。机器学习是人工智能的核心技术。深度学习推动了AI的快速发展。\"\n",
        "\n",
        "# 无重叠\n",
        "splitter_no_overlap = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=30,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "chunks_no_overlap = splitter_no_overlap.split_text(text)\n",
        "\n",
        "print(\"无重叠:\")\n",
        "for i, chunk in enumerate(chunks_no_overlap, 1):\n",
        "    print(f\"  块{i}: {chunk}\")\n",
        "print()\n",
        "\n",
        "# 有重叠\n",
        "splitter_with_overlap = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=30,\n",
        "    chunk_overlap=10,\n",
        ")\n",
        "chunks_with_overlap = splitter_with_overlap.split_text(text)\n",
        "\n",
        "print(\"有重叠（overlap=10）:\")\n",
        "for i, chunk in enumerate(chunks_with_overlap, 1):\n",
        "    print(f\"  块{i}: {chunk}\")\n",
        "print()\n",
        "\n",
        "print(\"注意：\")\n",
        "print(\"- 无重叠：每个块之间完全独立，可能导致信息被截断\")\n",
        "print(\"- 有重叠：块之间有共同部分，保证了上下文的连续性\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 完整流程示例\n",
        "\n",
        "现在让我们将Document Loader和Text Splitter结合起来，完成一个完整的文档处理流程。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【完整的文档处理流程】\n",
            "\n",
            "步骤1：加载文档\n",
            "  加载了 1 个文档\n",
            "  文档长度: 488 字符\n",
            "\n",
            "步骤2：分割文档\n",
            "  分割成 3 个块\n",
            "\n",
            "步骤3：查看前3个块\n",
            "\n",
            "块 1:\n",
            "  长度: 162 字符\n",
            "  起始位置: 0\n",
            "  内容: LangChain简介\n",
            "\n",
            "LangChain是一个强大的框架，专门用于构建基于大型语言模型（LLM）的应用程序。它提供了一套完整的工具链，帮助开发者快速构建智能应用。\n",
            "\n",
            "核心特性\n",
            "\n",
            "1. 模型集成\n",
            "L...\n",
            "\n",
            "块 2:\n",
            "  长度: 177 字符\n",
            "  起始位置: 164\n",
            "  内容: 2. 提示词管理\n",
            "LangChain提供了强大的提示词模板系统，支持动态变量、条件逻辑和少样本学习。\n",
            "\n",
            "3. 链式调用\n",
            "通过LCEL（LangChain Expression Language），可以...\n",
            "\n",
            "块 3:\n",
            "  长度: 144 字符\n",
            "  起始位置: 343\n",
            "  内容: 5. Agent框架\n",
            "LangChain提供了Agent框架，使模型能够自主决策和使用工具。\n",
            "\n",
            "应用场景\n",
            "\n",
            "- 问答系统\n",
            "- 文档分析\n",
            "- 代码生成\n",
            "- 对话机器人\n",
            "- 数据提取\n",
            "- 智能搜索\n",
            "\n",
            "总结...\n",
            "\n",
            "步骤4：验证重叠\n",
            "\n",
            "  未发现明显重叠（可能因为分割点在特殊位置）\n",
            "\n",
            "处理完成！这些分割后的文档现在可以用于：\n",
            "- 生成向量嵌入（Embeddings）\n",
            "- 存储到向量数据库\n",
            "- 用于RAG检索\n"
          ]
        }
      ],
      "source": [
        "print(\"【完整的文档处理流程】\")\n",
        "print()\n",
        "\n",
        "# 步骤1：创建一个较长的文档\n",
        "with open(\"../data/langchain_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\"\"LangChain简介\n",
        "\n",
        "LangChain是一个强大的框架，专门用于构建基于大型语言模型（LLM）的应用程序。它提供了一套完整的工具链，帮助开发者快速构建智能应用。\n",
        "\n",
        "核心特性\n",
        "\n",
        "1. 模型集成\n",
        "LangChain支持多种语言模型提供商，包括OpenAI、Anthropic、Google等。开发者可以轻松切换不同的模型。\n",
        "\n",
        "2. 提示词管理\n",
        "LangChain提供了强大的提示词模板系统，支持动态变量、条件逻辑和少样本学习。\n",
        "\n",
        "3. 链式调用\n",
        "通过LCEL（LangChain Expression Language），可以将多个组件链接在一起，构建复杂的处理流程。\n",
        "\n",
        "4. RAG支持\n",
        "LangChain内置了完整的RAG工具链，包括文档加载器、文本分割器、向量数据库集成等。\n",
        "\n",
        "5. Agent框架\n",
        "LangChain提供了Agent框架，使模型能够自主决策和使用工具。\n",
        "\n",
        "应用场景\n",
        "\n",
        "- 问答系统\n",
        "- 文档分析\n",
        "- 代码生成\n",
        "- 对话机器人\n",
        "- 数据提取\n",
        "- 智能搜索\n",
        "\n",
        "总结\n",
        "\n",
        "LangChain是构建LLM应用的最佳选择，它大大降低了开发难度，提高了开发效率。\n",
        "\"\"\")\n",
        "\n",
        "# 步骤2：加载文档\n",
        "print(\"步骤1：加载文档\")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"../data/langchain_intro.txt\", encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"  加载了 {len(docs)} 个文档\")\n",
        "print(f\"  文档长度: {len(docs[0].page_content)} 字符\")\n",
        "print()\n",
        "\n",
        "# 步骤3：分割文档\n",
        "print(\"步骤2：分割文档\")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,      # 每块200字符\n",
        "    chunk_overlap=50,    # 重叠50字符\n",
        "    add_start_index=True # 记录起始位置\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"  分割成 {len(split_docs)} 个块\")\n",
        "print()\n",
        "\n",
        "# 步骤4：查看分割结果\n",
        "print(\"步骤3：查看前3个块\")\n",
        "print()\n",
        "for i, doc in enumerate(split_docs[:3], 1):\n",
        "    print(f\"块 {i}:\")\n",
        "    print(f\"  长度: {len(doc.page_content)} 字符\")\n",
        "    print(f\"  起始位置: {doc.metadata.get('start_index', 'N/A')}\")\n",
        "    print(f\"  内容: {doc.page_content[:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"步骤4：验证重叠\")\n",
        "print()\n",
        "# 查看第1块和第2块的重叠部分\n",
        "if len(split_docs) >= 2:\n",
        "    chunk1 = split_docs[0].page_content\n",
        "    chunk2 = split_docs[1].page_content\n",
        "    \n",
        "    # 找出重叠部分（简化版）\n",
        "    overlap_found = False\n",
        "    for i in range(len(chunk1)):\n",
        "        if chunk2.startswith(chunk1[i:]):\n",
        "            overlap = chunk1[i:]\n",
        "            print(f\"块1和块2的重叠部分（{len(overlap)}字符）:\")\n",
        "            print(f\"  '{overlap}'\")\n",
        "            overlap_found = True\n",
        "            break\n",
        "    \n",
        "    if not overlap_found:\n",
        "        print(\"  未发现明显重叠（可能因为分割点在特殊位置）\")\n",
        "\n",
        "print()\n",
        "print(\"处理完成！这些分割后的文档现在可以用于：\")\n",
        "print(\"- 生成向量嵌入（Embeddings）\")\n",
        "print(\"- 存储到向量数据库\")\n",
        "print(\"- 用于RAG检索\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 实战项目：智能文档处理系统\n",
        "\n",
        "构建一个完整的文档处理系统，支持：\n",
        "1. 多种格式的文档加载\n",
        "2. 智能分割策略\n",
        "3. 元数据管理\n",
        "4. 统计分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【智能文档处理系统】\n",
            "\n",
            "文档处理器类定义完成！\n"
          ]
        }
      ],
      "source": [
        "print(\"【智能文档处理系统】\")\n",
        "print()\n",
        "\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ProcessingStats:\n",
        "    \"\"\"处理统计信息\"\"\"\n",
        "    total_docs: int\n",
        "    total_chunks: int\n",
        "    avg_chunk_size: float\n",
        "    min_chunk_size: int\n",
        "    max_chunk_size: int\n",
        "    file_types: Dict[str, int]\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"智能文档处理器\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        \"\"\"\n",
        "        初始化文档处理器\n",
        "        \n",
        "        Args:\n",
        "            chunk_size: 块大小\n",
        "            chunk_overlap: 重叠大小\n",
        "        \"\"\"\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            add_start_index=True\n",
        "        )\n",
        "        self.documents = []\n",
        "        self.chunks = []\n",
        "    \n",
        "    def load_text(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"加载文本文件\"\"\"\n",
        "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        docs = loader.load()\n",
        "        # 添加文件类型到元数据\n",
        "        for doc in docs:\n",
        "            doc.metadata[\"file_type\"] = \"text\"\n",
        "        return docs\n",
        "    \n",
        "    def load_csv(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"加载CSV文件\"\"\"\n",
        "        loader = CSVLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        for doc in docs:\n",
        "            doc.metadata[\"file_type\"] = \"csv\"\n",
        "        return docs\n",
        "    \n",
        "    def load_json(self, file_path: str, jq_schema: str = \".\") -> List[Document]:\n",
        "        \"\"\"加载JSON文件\"\"\"\n",
        "        loader = JSONLoader(\n",
        "            file_path=file_path,\n",
        "            jq_schema=jq_schema,\n",
        "            text_content=False\n",
        "        )\n",
        "        docs = loader.load()\n",
        "        for doc in docs:\n",
        "            doc.metadata[\"file_type\"] = \"json\"\n",
        "        return docs\n",
        "    \n",
        "    def add_documents(self, docs: List[Document]):\n",
        "        \"\"\"添加文档到处理器\"\"\"\n",
        "        self.documents.extend(docs)\n",
        "    \n",
        "    def process(self):\n",
        "        \"\"\"处理所有文档：分割成块\"\"\"\n",
        "        self.chunks = self.text_splitter.split_documents(self.documents)\n",
        "        return self.chunks\n",
        "    \n",
        "    def get_stats(self) -> ProcessingStats:\n",
        "        \"\"\"获取处理统计信息\"\"\"\n",
        "        if not self.chunks:\n",
        "            return None\n",
        "        \n",
        "        chunk_sizes = [len(chunk.page_content) for chunk in self.chunks]\n",
        "        \n",
        "        # 统计文件类型\n",
        "        file_types = {}\n",
        "        for doc in self.documents:\n",
        "            file_type = doc.metadata.get(\"file_type\", \"unknown\")\n",
        "            file_types[file_type] = file_types.get(file_type, 0) + 1\n",
        "        \n",
        "        return ProcessingStats(\n",
        "            total_docs=len(self.documents),\n",
        "            total_chunks=len(self.chunks),\n",
        "            avg_chunk_size=sum(chunk_sizes) / len(chunk_sizes),\n",
        "            min_chunk_size=min(chunk_sizes),\n",
        "            max_chunk_size=max(chunk_sizes),\n",
        "            file_types=file_types\n",
        "        )\n",
        "    \n",
        "    def search_chunks(self, keyword: str) -> List[Document]:\n",
        "        \"\"\"搜索包含关键词的块\"\"\"\n",
        "        return [\n",
        "            chunk for chunk in self.chunks \n",
        "            if keyword.lower() in chunk.page_content.lower()\n",
        "        ]\n",
        "    \n",
        "    def filter_by_metadata(self, key: str, value: str) -> List[Document]:\n",
        "        \"\"\"根据元数据筛选块\"\"\"\n",
        "        return [\n",
        "            chunk for chunk in self.chunks\n",
        "            if chunk.metadata.get(key) == value\n",
        "        ]\n",
        "\n",
        "print(\"文档处理器类定义完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【使用文档处理系统】\n",
            "\n",
            "步骤1：加载多种格式的文档\n",
            "  加载文本文件: 1 个文档\n",
            "  加载CSV文件: 3 个文档\n",
            "  加载JSON文件: 3 个文档\n",
            "  总计: 7 个文档\n",
            "\n",
            "步骤2：分割文档\n",
            "  生成了 8 个文本块\n",
            "\n",
            "步骤3：统计信息\n",
            "  原始文档数: 7\n",
            "  文本块数: 8\n",
            "  平均块大小: 93.2 字符\n",
            "  最小块大小: 25 字符\n",
            "  最大块大小: 285 字符\n",
            "  文件类型分布: {'text': 1, 'csv': 3, 'json': 3}\n",
            "\n",
            "步骤4：搜索功能\n",
            "  搜索'LangChain'找到 2 个块\n",
            "  第一个结果: LangChain简介\n",
            "\n",
            "LangChain是一个强大的框架，专门用于构建基于大型语言模型（LLM）的应用程序。它提供了一套完整的工具链，帮助开发者快速构建智能应用。\n",
            "\n",
            "核心特性\n",
            "\n",
            "1. 模型集成\n",
            "L...\n",
            "\n",
            "步骤5：元数据筛选\n",
            "  文本文件的块数: 2\n",
            "  CSV文件的块数: 3\n",
            "  JSON文件的块数: 3\n",
            "\n",
            "步骤6：查看示例块（来自不同文件类型）\n",
            "\n",
            "TEXT 文件示例:\n",
            "  内容: LangChain简介\n",
            "\n",
            "LangChain是一个强大的框架，专门用于构建基于大型语言模型（LLM）的应用程序。它提供了一套完整的工具链，帮助开发者快速构建智能应用。\n",
            "\n",
            "核心特性\n",
            "\n",
            "1. 模型集成\n",
            "LangChain支持多种语言模型提供商，包括OpenAI、Anthropic、Google等。开发者...\n",
            "  元数据: {'source': '../data/langchain_intro.txt', 'file_type': 'text', 'start_index': 0}\n",
            "\n",
            "CSV 文件示例:\n",
            "  内容: 产品名称: 笔记本电脑\n",
            "价格: 5999\n",
            "描述: 高性能办公笔记本...\n",
            "  元数据: {'source': '../data/products.csv', 'row': 0, 'file_type': 'csv', 'start_index': 0}\n",
            "\n",
            "JSON 文件示例:\n",
            "  内容: {\"name\": \"\\u5f20\\u4e09\", \"age\": 25, \"city\": \"\\u5317\\u4eac\"}...\n",
            "  元数据: {'source': '/home/iip/tp/038-project/rag/data/users.json', 'seq_num': 1, 'file_type': 'json', 'start_index': 0}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"【使用文档处理系统】\")\n",
        "print()\n",
        "\n",
        "# 1. 创建处理器\n",
        "processor = DocumentProcessor(chunk_size=300, chunk_overlap=50)\n",
        "\n",
        "print(\"步骤1：加载多种格式的文档\")\n",
        "# 加载文本文件\n",
        "text_docs = processor.load_text(\"../data/langchain_intro.txt\")\n",
        "processor.add_documents(text_docs)\n",
        "print(f\"  加载文本文件: {len(text_docs)} 个文档\")\n",
        "\n",
        "# 加载CSV文件\n",
        "csv_docs = processor.load_csv(\"../data/products.csv\")\n",
        "processor.add_documents(csv_docs)\n",
        "print(f\"  加载CSV文件: {len(csv_docs)} 个文档\")\n",
        "\n",
        "# 加载JSON文件\n",
        "json_docs = processor.load_json(\"../data/users.json\", jq_schema=\".users[]\")\n",
        "processor.add_documents(json_docs)\n",
        "print(f\"  加载JSON文件: {len(json_docs)} 个文档\")\n",
        "\n",
        "print(f\"  总计: {len(processor.documents)} 个文档\")\n",
        "print()\n",
        "\n",
        "# 2. 处理文档\n",
        "print(\"步骤2：分割文档\")\n",
        "chunks = processor.process()\n",
        "print(f\"  生成了 {len(chunks)} 个文本块\")\n",
        "print()\n",
        "\n",
        "# 3. 获取统计信息\n",
        "print(\"步骤3：统计信息\")\n",
        "stats = processor.get_stats()\n",
        "print(f\"  原始文档数: {stats.total_docs}\")\n",
        "print(f\"  文本块数: {stats.total_chunks}\")\n",
        "print(f\"  平均块大小: {stats.avg_chunk_size:.1f} 字符\")\n",
        "print(f\"  最小块大小: {stats.min_chunk_size} 字符\")\n",
        "print(f\"  最大块大小: {stats.max_chunk_size} 字符\")\n",
        "print(f\"  文件类型分布: {stats.file_types}\")\n",
        "print()\n",
        "\n",
        "# 4. 搜索功能\n",
        "print(\"步骤4：搜索功能\")\n",
        "search_results = processor.search_chunks(\"LangChain\")\n",
        "print(f\"  搜索'LangChain'找到 {len(search_results)} 个块\")\n",
        "if search_results:\n",
        "    print(f\"  第一个结果: {search_results[0].page_content[:100]}...\")\n",
        "print()\n",
        "\n",
        "# 5. 元数据筛选\n",
        "print(\"步骤5：元数据筛选\")\n",
        "text_chunks = processor.filter_by_metadata(\"file_type\", \"text\")\n",
        "print(f\"  文本文件的块数: {len(text_chunks)}\")\n",
        "\n",
        "csv_chunks = processor.filter_by_metadata(\"file_type\", \"csv\")\n",
        "print(f\"  CSV文件的块数: {len(csv_chunks)}\")\n",
        "\n",
        "json_chunks = processor.filter_by_metadata(\"file_type\", \"json\")\n",
        "print(f\"  JSON文件的块数: {len(json_chunks)}\")\n",
        "print()\n",
        "\n",
        "# 6. 查看示例块\n",
        "print(\"步骤6：查看示例块（来自不同文件类型）\")\n",
        "print()\n",
        "\n",
        "for file_type in [\"text\", \"csv\", \"json\"]:\n",
        "    chunks_of_type = processor.filter_by_metadata(\"file_type\", file_type)\n",
        "    if chunks_of_type:\n",
        "        chunk = chunks_of_type[0]\n",
        "        print(f\"{file_type.upper()} 文件示例:\")\n",
        "        print(f\"  内容: {chunk.page_content[:150]}...\")\n",
        "        print(f\"  元数据: {chunk.metadata}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 总结与最佳实践\n",
        "\n",
        "### 核心概念回顾\n",
        "\n",
        "1. **Document对象**\n",
        "   - `page_content`：文本内容\n",
        "   - `metadata`：元数据\n",
        "   - `id`：可选标识符\n",
        "\n",
        "2. **Document Loaders**\n",
        "   - 统一接口：`load()` 和 `load_and_split()`\n",
        "   - 支持多种格式：TXT、CSV、JSON、PDF、DOCX等\n",
        "   - 返回标准的Document对象\n",
        "\n",
        "3. **Text Splitters**\n",
        "   - RecursiveCharacterTextSplitter（推荐）\n",
        "   - 关键参数：chunk_size、chunk_overlap\n",
        "   - 保持文本结构的完整性\n",
        "\n",
        "### 最佳实践\n",
        "\n",
        "#### 1. 选择合适的chunk_size\n",
        "\n",
        "| 应用场景 | 推荐大小 | 说明 |\n",
        "|---------|---------|------|\n",
        "| 问答系统 | 500-1000 | 平衡精确度和上下文 |\n",
        "| 语义搜索 | 200-500 | 更精确的匹配 |\n",
        "| 文档摘要 | 1000-2000 | 需要更多上下文 |\n",
        "| 代码分析 | 按函数/类分割 | 使用结构化分割器 |\n",
        "\n",
        "#### 2. 设置合理的chunk_overlap\n",
        "\n",
        "- 一般为chunk_size的10%-20%\n",
        "- 避免重要信息被截断\n",
        "- 不要过大，会浪费存储空间\n",
        "\n",
        "#### 3. 利用元数据\n",
        "\n",
        "- 记录文档来源\n",
        "- 添加自定义标签\n",
        "- 使用`add_start_index=True`跟踪位置\n",
        "- 便于后续筛选和溯源\n",
        "\n",
        "#### 4. 选择合适的分割器\n",
        "\n",
        "- **通用文本**：RecursiveCharacterTextSplitter\n",
        "- **Markdown**：MarkdownTextSplitter\n",
        "- **代码**：Language-specific code splitters\n",
        "- **HTML**：HTMLTextSplitter\n",
        "\n",
        "### 常见陷阱\n",
        "\n",
        "1. **chunk_size过大**：检索不精确，超出模型上下文窗口\n",
        "2. **chunk_size过小**：丢失上下文，理解困难\n",
        "3. **没有重叠**：重要信息可能被截断\n",
        "4. **忽略元数据**：难以追踪来源和管理"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
